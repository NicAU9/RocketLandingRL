{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2435681-6d88-475d-8f29-3deab34f7259",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1344b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0349904-9a60-4b3f-bf19-017259996b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_render_loop(agent, env, transpose=False, print_step=False):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    clock = pygame.time.Clock()\n",
    "    fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "    pygame.display.init()\n",
    "    size = env.render().shape[:2]\n",
    "    if transpose:\n",
    "        size = size[::-1]\n",
    "\n",
    "    display = pygame.display.set_mode(size)\n",
    "\n",
    "    term = trunc = quit = False\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    if print_step:\n",
    "        print(\"initial state\")\n",
    "        print(\"\\tobservation\", obs)\n",
    "        print(\"\\tinfo\", info)\n",
    "\n",
    "    while not (term or trunc or quit):\n",
    "        quit = pygame.key.get_pressed()[pygame.K_q]\n",
    "\n",
    "        action = agent(obs)\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "\n",
    "        pygame.event.pump()\n",
    "\n",
    "        step += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        if print_step:\n",
    "            print(\"step\", step)\n",
    "            print(\"\\taction\", action)\n",
    "            print(\"\\tobservation\", obs)\n",
    "            print(\"\\treward\", reward)\n",
    "            print(\"\\tterminated\", term)\n",
    "            print(\"\\ttruncated\", trunc)\n",
    "            print(\"\\tinfo\", info)\n",
    "\n",
    "        frame = env.render()\n",
    "        if transpose:\n",
    "            frame = frame.transpose((1, 0, 2))\n",
    "\n",
    "        pygame.surfarray.blit_array(display, frame)\n",
    "        pygame.display.flip()\n",
    "        clock.tick(fps)\n",
    "\n",
    "    if print_step:\n",
    "        print(\"Total reward\", total_reward)\n",
    "\n",
    "    pygame.display.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b85d5d",
   "metadata": {},
   "source": [
    "# Rocket Landing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3a0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pygame.init()\n",
    "\n",
    "WIDTH, HEIGHT = 1200, 900\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "gravity = 0.5\n",
    "mass = 1\n",
    "\n",
    "# screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "# pygame.display.set_caption(\"Rocket Landing Game\")\n",
    "# clock = pygame.time.Clock()\n",
    "\n",
    "class Rocket:\n",
    "    def __init__(self, state):\n",
    "        # state: ['x', 'y', 'vx', 'vy', 'theta', 'w']\n",
    "        # Cmd: ['Thrust', 'delta']\n",
    "\n",
    "        self.image = pygame.image.load(\"rocket.png\")\n",
    "        # self.width, self.height = self.image.get_size()\n",
    "        self.width = 20  \n",
    "        self.height = 60\n",
    "\n",
    "        # State Init\n",
    "        self.state = state\n",
    "\n",
    "        # Cmd Init\n",
    "        self.thruster_angle = 0\n",
    "        self.thrust = 0\n",
    "\n",
    "        # Thrust position from direct kinematic\n",
    "        self.thrust_x = self.state[0] + np.sin(np.radians(self.state[4])) * self.height / 2\n",
    "        self.thrust_y = self.state[1] + np.cos(np.radians(self.state[4])) * -self.height / 2\n",
    "\n",
    "        # Kinematic params\n",
    "        self.mass = mass\n",
    "        self.g = -gravity\n",
    "        self.I = 50 #1/12 * self.mass * (self.width**2 + self.height**2)\n",
    "\n",
    "        self.thrust_on = False\n",
    "\n",
    "    def update_state(self, cmd, dt):\n",
    "        x, y, vx, vy, theta, w = self.state   \n",
    "        T, delta = np.array(cmd)\n",
    "\n",
    "        self.thruster_angle = theta - delta\n",
    "        self.thrust = T\n",
    "\n",
    "        # Forces applied by thrust\n",
    "        Fx = np.sin(np.radians(self.thruster_angle)) * T\n",
    "        Fy = np.cos(np.radians(self.thruster_angle)) * T\n",
    "\n",
    "        # Acceleration from thrust\n",
    "        ax = Fx / self.mass\n",
    "        ay = -Fy / self.mass - self.g\n",
    "\n",
    "        ## Update linear states\n",
    "        # Update speed\n",
    "        vx += ax * dt * 30 \n",
    "        vy += ay * dt * 30\n",
    "\n",
    "        # Update position\n",
    "        x += vx * dt * 30\n",
    "        y += vy * dt * 30\n",
    "\n",
    "        # Angular acceleration due to thrust\n",
    "        self.thrust_x = (x + np.sin(np.radians(theta)) * -self.height / 2)\n",
    "        self.thrust_y = (y + np.cos(np.radians(theta)) * self.height / 2)\n",
    "\n",
    "        lever_arm_x = self.thrust_x - x\n",
    "        lever_arm_y = self.thrust_y - y\n",
    "\n",
    "        alpha = (lever_arm_x * Fy + lever_arm_y * Fx) / self.I\n",
    "\n",
    "        ## Update angular states\n",
    "        # Update speed\n",
    "        w += alpha * dt * 30\n",
    "\n",
    "        # Update angle\n",
    "        theta += w * dt * 30\n",
    "\n",
    "        self.state = np.array([x, y, vx, vy, theta, w])\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def draw(self, screen):\n",
    "        x, y, _, _, theta, _ = self.state\n",
    "\n",
    "        rotated_image = pygame.transform.rotate(self.image, -theta)\n",
    "        new_rect = rotated_image.get_rect(center=(x, y))\n",
    "        screen.blit(rotated_image, new_rect.topleft)\n",
    "\n",
    "        thrust_length = self.thrust * 50\n",
    "        thrust_x = self.thrust_x - thrust_length * np.sin(np.radians(self.thruster_angle))\n",
    "        thrust_y = self.thrust_y + thrust_length * np.cos(np.radians(self.thruster_angle))\n",
    "\n",
    "        pygame.draw.line(screen, (255, 0, 0), (self.thrust_x, self.thrust_y), (thrust_x, thrust_y), 3)\n",
    "\n",
    "    def check_collision(self, screen, target):\n",
    "        _, y, _, _, _, _ = self.state\n",
    "\n",
    "        # Floor\n",
    "        floor_y = target[0] + self.height + 80\n",
    "        pygame.draw.rect(screen, (139, 69, 19), (0, floor_y, WIDTH, HEIGHT - y))  # Brown-colored floor\n",
    "\n",
    "        tower_width = 10\n",
    "        tower_height = self.height + 40\n",
    "        tower_x = target[0] + self.width + tower_width\n",
    "        tower_y = floor_y - tower_height\n",
    "        pygame.draw.rect(screen, (150, 150, 150), (tower_x, tower_y, tower_width, tower_height))\n",
    "\n",
    "        # Ground collision\n",
    "        # if y + self.height / 2  >= floor_y:\n",
    "        #     self.state[1] = floor_y - self.height / 2\n",
    "            \n",
    "        #     if np.isclose(self.state, target, atol=5).all():\n",
    "        #         return \"win\"  \n",
    "        #     else:\n",
    "        #         return \"lose\"  \n",
    "        \n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436042b4",
   "metadata": {},
   "source": [
    "# Gym Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3096f",
   "metadata": {},
   "source": [
    "Les espaces d'observations et d'action sont Ã©tablie selon la physique du jeu (selon le nombre de pixel et le nombre de FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0891fb-ed5f-4372-b607-03c963be0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocketLandingEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, cible = [WIDTH // 2, HEIGHT - HEIGHT // 4, 0.0, 0.0, 0.0, 0.0], world_size=(WIDTH, HEIGHT), render_mode=None):\n",
    "        super(RocketLandingEnv, self).__init__()\n",
    "\n",
    "        self.world_size = self.HEIGHT, self.WIDTH = world_size\n",
    "\n",
    "        # State Init\n",
    "        self.state = np.array([0, 0, 0, 0, 0, 0])\n",
    "        self.cible = cible\n",
    "\n",
    "        self.rocket = Rocket(self.state)\n",
    "\n",
    "        # Cmd Init\n",
    "        self.cmd = np.zeros(2)\n",
    "        self.dt = 1 / self.metadata[\"render_fps\"]\n",
    "        self.t = 0\n",
    "\n",
    "        # State and action spaces\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low= np.array([0, 0, -20, -20, -180, -10]),\n",
    "            high= np.array([self.WIDTH, self.HEIGHT, 20, 20, 180, 10]), \n",
    "            shape=(6,),\n",
    "            dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low=np.array([gravity * mass - 0.7, -15]), high=np.array([gravity * mass + 0.7, 15]), shape=(2,), dtype=np.float32)\n",
    "\n",
    "        self.game_over = \"continue\"\n",
    "        self.init_offset = 100 # Pixels\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    def _dynamic_step(self, action):\n",
    "        self.t += self.dt\n",
    "        self.cmd = action\n",
    "        \n",
    "        self.rocket.update_state(self.cmd, self.dt)\n",
    "\n",
    "        self.state = self.rocket.get_state()\n",
    "        \n",
    "    def _g(self):\n",
    "        u = self.cmd\n",
    "        dstate = self.cible - self._get_observation()\n",
    "        \n",
    "        Q = np.diag([1., 1., 0.1, 0.1, 5., 0.1]) # x, y, vx, vy, theta, w\n",
    "        R = np.diag([0.001, 0.001]) # T, delta\n",
    "\n",
    "        cout = (np.dot(dstate.T, np.dot(Q , dstate)) + np.dot(u.T, np.dot(R ,u))) * self.dt\n",
    "\n",
    "        if (np.linalg.norm(dstate) < 2.5):\n",
    "            cout = 0.0\n",
    "            \n",
    "        cout += 100 * (not self.observation_space.contains(self.state))\n",
    "\n",
    "        return cout \n",
    "\n",
    "    def _get_observation(self):\n",
    "        return (self.state).astype(np.float32)\n",
    "\n",
    "    def _should_truncate(self):\n",
    "        return not self.observation_space.contains(self._get_observation())\n",
    "\n",
    "    def _should_terminate(self):\n",
    "\n",
    "        dx = self.cible - self._get_observation()\n",
    "        return (np.linalg.norm(dx) < 2.5)\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"t\": self.t,\n",
    "            \"distance\": np.linalg.norm(self.cible - self.state)\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        self.state = np.array([np.random.uniform(WIDTH // 2 - self.init_offset, WIDTH // 2 + self.init_offset), HEIGHT // 4, 0, 0, 0, 0])\n",
    "        #self.state = np.array([WIDTH // 2 , HEIGHT // 4, 0, 0, 0, 0])\n",
    "        self.rocket = Rocket(self.state)\n",
    "        observation = self._get_observation()\n",
    "\n",
    "        self.cmd = np.zeros(2)\n",
    "\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self._dynamic_step(action)\n",
    "        observation = self._get_observation()\n",
    "        terminate = self._should_terminate()\n",
    "        truncate = self._should_truncate()\n",
    "\n",
    "        reward = -self._g()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        return observation, reward, terminate, truncate, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "\n",
    "            size = pygame.Vector2(self.world_size)\n",
    "            surface = pygame.Surface(size, flags=pygame.SRCALPHA)\n",
    "\n",
    "            self.game_over = self.rocket.check_collision(surface, self.cible)\n",
    "\n",
    "            self.rocket.draw(surface)\n",
    "        \n",
    "            return pygame.surfarray.pixels3d(surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0b0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5388dc9-bf81-4221-91fb-ed290a5c9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(id=\"RocketLandingEnv-v0\", entry_point=\"__main__:RocketLandingEnv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5935e88-1edf-4787-8bfe-d479dcbaeb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"RocketLandingEnv-v0\", render_mode=\"rgb_array\", max_episode_steps=1000)\n",
    "\n",
    "def human_agent(_obs):\n",
    "    keys = pygame.key.get_pressed()\n",
    "    pygame.event.pump()\n",
    "\n",
    "    delta_input = 0\n",
    "    trust_input = 0\n",
    "\n",
    "    if keys[pygame.K_UP]:\n",
    "        if not env.rocket.thrust_on:\n",
    "            env.rocket.thrust_on = True\n",
    "    else:\n",
    "        env.rocket.thrust_on = False\n",
    "\n",
    "    if env.rocket.thrust_on:\n",
    "        trust_input = gravity * mass + 0.2\n",
    "    else:\n",
    "        trust_input = 0\n",
    "\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        delta_input = -15\n",
    "    if keys[pygame.K_RIGHT]:\n",
    "        delta_input = 15\n",
    "\n",
    "    action = np.array([trust_input, delta_input])\n",
    "\n",
    "    return action\n",
    "\n",
    "human_render_loop(human_agent, env, print_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba883aeb",
   "metadata": {},
   "source": [
    "# PPO Agent Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6381eca",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2052767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(-1, 1, shape=env.observation_space.shape, dtype=env.observation_space.dtype)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        low = self.env.observation_space.low\n",
    "        high = self.env.observation_space.high\n",
    "        return -1 + 2 / (high - low) * (high - obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd976df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.action_space = gym.spaces.Box(-1, 1, shape=env.action_space.shape, dtype=env.action_space.dtype)\n",
    "\n",
    "    def action(self, act):\n",
    "        low = self.env.action_space.low\n",
    "        high = self.env.action_space.high\n",
    "        return low + (high-low) / 2 * (1 - act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991b910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleReward(gym.RewardWrapper):\n",
    "\n",
    "    def __init__(self, env, factor):\n",
    "        super().__init__(env)\n",
    "        self.factor = factor\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return reward * self.factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f01c0c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "133d6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"RocketLandingEnv-v0\", render_mode=\"rgb_array\", max_episode_steps=1000)\n",
    "env = NormalizeObservation(env)\n",
    "env = NormalizeAction(env)\n",
    "env = ScaleReward(env, factor=1/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28719423-6a16-40bf-8179-dc4baefbbea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ppo_rocket_tensorboard\\PPO_19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -85.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 853      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\n\u001b[0;32m      5\u001b[0m     env,\n\u001b[0;32m      6\u001b[0m     best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_rocket_tensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\utilisateur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\utilisateur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\utilisateur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:223\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Normalization does not make sense if mini batchsize == 1, see GH issue #325\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_advantage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(advantages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 223\u001b[0m     advantages \u001b[38;5;241m=\u001b[39m (advantages \u001b[38;5;241m-\u001b[39m advantages\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m (\u001b[43madvantages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# ratio between old and new policy, should be one at the first iteration\u001b[39;00m\n\u001b[0;32m    226\u001b[0m ratio \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mexp(log_prob \u001b[38;5;241m-\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mold_log_prob)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    best_model_save_path=\"best_model\",\n",
    "    log_path=\"logs\",\n",
    "    eval_freq=10000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, n_epochs=100, tensorboard_log=\"ppo_rocket_tensorboard\")\n",
    "model.learn(total_timesteps=500_000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39b179",
   "metadata": {},
   "source": [
    "# Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf85820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"Best_Model/Rocket_RL_Rand300.zip\")\n",
    "def agent(obs):\n",
    "    return model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "human_render_loop(agent, env, print_step=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb360b",
   "metadata": {},
   "source": [
    "## Train from a model\n",
    "Changer l'environnement pour augmenter le dÃ©callage par rapport Ã  la cible et rÃ©entrainer en partant du model prÃ©cÃ©dent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "266f99d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ppo_rocket_tensorboard\\PPO_24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -25.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 2452     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest_Model/Rocket_RL_Rand300.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m=\u001b[39menv)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:282\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:414\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    412\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 414\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep\n\u001b[0;32m    417\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep\n",
      "File \u001b[1;32mc:\\Users\\nicau\\Desktop\\src\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:106\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"Best_Model/Rocket_RL_Rand300.zip\", env=env)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=500_000, callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
